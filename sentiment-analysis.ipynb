{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":108235,"databundleVersionId":13123466,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"35d82544","cell_type":"markdown","source":"#  Text Sentiment Analysis\n## Exploratory Data Analysis (EDA)\n\nThis notebook performs comprehensive exploratory data analysis on the text sentiment classification dataset.","metadata":{}},{"id":"464272cb","cell_type":"code","source":"# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud\nimport re\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set style for better visualizations\nplt.style.use('default')\nsns.set_palette(\"husl\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7a735f28","cell_type":"markdown","source":"## 1. Data Loading and Initial Exploration","metadata":{}},{"id":"d0a4d97e","cell_type":"code","source":"# Load the datasets\ntrain_df = pd.read_csv('/kaggle/input/mc-datathon-2025-sentiment-analysis/train.csv')\ntest_df = pd.read_csv('/kaggle/input/mc-datathon-2025-sentiment-analysis/test.csv')\n\nprint(\" Dataset Overview\")\nprint(\"=\"*50)\nprint(f\"Training set shape: {train_df.shape}\")\nprint(f\"Test set shape: {test_df.shape}\")\nprint(\"\\n Training Data Info:\")\nprint(train_df.info())\nprint(\"\\n Test Data Info:\")\nprint(test_df.info())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"1ed99c88","cell_type":"code","source":"# Display first few rows\nprint(\" First 5 rows of Training Data:\")\nprint(train_df.head())\nprint(\"\\n First 5 rows of Test Data:\")\nprint(test_df.head())\n\n# Check columns\nprint(f\"\\n Training data columns: {list(train_df.columns)}\")\nprint(f\" Test data columns: {list(test_df.columns)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"72c32c1b","cell_type":"code","source":"# Check for missing values\nprint(\" Missing Values Analysis:\")\nprint(\"=\"*30)\nprint(\"Training Data:\")\nprint(train_df.isnull().sum())\nprint(\"\\nTest Data:\")\nprint(test_df.isnull().sum())\n\n# Check for duplicates\nprint(f\"\\n Duplicate rows in training data: {train_df.duplicated().sum()}\")\nprint(f\" Duplicate rows in test data: {test_df.duplicated().sum()}\")\n\n# Check unique values in sentiment column (only for training data)\nif 'sentiment' in train_df.columns:\n    print(f\"\\n Unique sentiments (before cleaning): {train_df['sentiment'].unique()}\")\n    print(f\"Number of unique sentiments: {train_df['sentiment'].nunique()}\")\n    \n    # Check for missing sentiment values\n    missing_sentiment_count = train_df['sentiment'].isnull().sum()\n    if missing_sentiment_count > 0:\n        print(f\" Missing sentiment values: {missing_sentiment_count}\")\n        print(\"These will be handled during preprocessing...\")\n    else:\n        print(\" No missing sentiment values\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f3f57418","cell_type":"markdown","source":"## 2. Target Variable Analysis (Sentiment Distribution)","metadata":{}},{"id":"9af03df0","cell_type":"code","source":"# Analyze sentiment distribution\nif 'sentiment' in train_df.columns:\n    sentiment_counts = train_df['sentiment'].value_counts()\n    print(\" Sentiment Distribution:\")\n    print(sentiment_counts)\n    print(f\"\\nPercentages:\")\n    print(train_df['sentiment'].value_counts(normalize=True) * 100)\n\n    # Visualize sentiment distribution\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n    # Bar plot\n    sentiment_counts.plot(kind='bar', ax=ax1, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n    ax1.set_title('Sentiment Distribution (Count)', fontsize=14, fontweight='bold')\n    ax1.set_xlabel('Sentiment')\n    ax1.set_ylabel('Count')\n    ax1.tick_params(axis='x', rotation=45)\n\n    # Pie chart\n    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n    ax2.pie(sentiment_counts.values, labels=sentiment_counts.index, autopct='%1.1f%%', \n            colors=colors, startangle=90)\n    ax2.set_title('Sentiment Distribution (Percentage)', fontsize=14, fontweight='bold')\n\n    plt.tight_layout()\n    plt.show()\nelse:\n    print(\"No sentiment column found in training data\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f976a339","cell_type":"markdown","source":"## 3. Text Length Analysis","metadata":{}},{"id":"5b5c487e","cell_type":"code","source":"# Calculate text statistics\ntrain_df['text_length'] = train_df['text'].str.len()\ntrain_df['word_count'] = train_df['text'].str.split().str.len()\ntest_df['text_length'] = test_df['text'].str.len()\ntest_df['word_count'] = test_df['text'].str.split().str.len()\n\nprint(\" Text Length Statistics:\")\nprint(\"=\"*30)\nprint(\"Training Data:\")\nprint(train_df[['text_length', 'word_count']].describe())\nprint(\"\\nTest Data:\")\nprint(test_df[['text_length', 'word_count']].describe())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"984ce0ac","cell_type":"code","source":"# Visualize text length distribution by sentiment\nif 'sentiment' in train_df.columns:\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n\n    # Character length distribution\n    for sentiment in train_df['sentiment'].unique():\n        subset = train_df[train_df['sentiment'] == sentiment]\n        axes[0, 0].hist(subset['text_length'], alpha=0.7, label=sentiment, bins=30)\n    axes[0, 0].set_title('Character Length Distribution by Sentiment')\n    axes[0, 0].set_xlabel('Character Length')\n    axes[0, 0].set_ylabel('Frequency')\n    axes[0, 0].legend()\n\n    # Word count distribution\n    for sentiment in train_df['sentiment'].unique():\n        subset = train_df[train_df['sentiment'] == sentiment]\n        axes[0, 1].hist(subset['word_count'], alpha=0.7, label=sentiment, bins=30)\n    axes[0, 1].set_title('Word Count Distribution by Sentiment')\n    axes[0, 1].set_xlabel('Word Count')\n    axes[0, 1].set_ylabel('Frequency')\n    axes[0, 1].legend()\n\n    # Box plots\n    sns.boxplot(data=train_df, x='sentiment', y='text_length', ax=axes[1, 0])\n    axes[1, 0].set_title('Character Length by Sentiment (Box Plot)')\n\n    sns.boxplot(data=train_df, x='sentiment', y='word_count', ax=axes[1, 1])\n    axes[1, 1].set_title('Word Count by Sentiment (Box Plot)')\n\n    plt.tight_layout()\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a9908d88","cell_type":"markdown","source":"## 4. Text Content Analysis","metadata":{}},{"id":"65de8c57","cell_type":"code","source":"# Function to clean text for analysis\ndef clean_text_for_analysis(text):\n    # Convert to lowercase and remove special characters\n    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n    return text\n\n# Sample some texts to see patterns\nprint(\" Sample Texts by Sentiment:\")\nprint(\"=\"*50)\nif 'sentiment' in train_df.columns:\n    for sentiment in train_df['sentiment'].unique():\n        print(f\"\\n{sentiment} Examples:\")\n        samples = train_df[train_df['sentiment'] == sentiment]['text'].head(3)\n        for i, text in enumerate(samples, 1):\n            print(f\"{i}. {text[:100]}...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3983704b","cell_type":"code","source":"# Most common words by sentiment\ndef get_most_common_words(texts, n=20):\n    all_words = []\n    for text in texts:\n        cleaned = clean_text_for_analysis(str(text))\n        words = cleaned.split()\n        # Filter out very short words\n        words = [word for word in words if len(word) > 2]\n        all_words.extend(words)\n    return Counter(all_words).most_common(n)\n\nprint(\" Most Common Words by Sentiment:\")\nprint(\"=\"*40)\nif 'sentiment' in train_df.columns:\n    for sentiment in train_df['sentiment'].unique():\n        texts = train_df[train_df['sentiment'] == sentiment]['text']\n        common_words = get_most_common_words(texts, 15)\n\n        for word, count in common_words:\n            print(f\"  {word}: {count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d8e349d2","cell_type":"markdown","source":"## 5. Comparative Analysis: Train vs Test","metadata":{}},{"id":"ccc49f5b","cell_type":"code","source":"# Compare distributions between train and test\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\n\n# Character length comparison\naxes[0, 0].hist(train_df['text_length'], alpha=0.7, label='Train', bins=30, color='blue')\naxes[0, 0].hist(test_df['text_length'], alpha=0.7, label='Test', bins=30, color='red')\naxes[0, 0].set_title('Character Length Distribution: Train vs Test')\naxes[0, 0].set_xlabel('Character Length')\naxes[0, 0].set_ylabel('Frequency')\naxes[0, 0].legend()\n\n# Word count comparison\naxes[0, 1].hist(train_df['word_count'], alpha=0.7, label='Train', bins=30, color='blue')\naxes[0, 1].hist(test_df['word_count'], alpha=0.7, label='Test', bins=30, color='red')\naxes[0, 1].set_title('Word Count Distribution: Train vs Test')\naxes[0, 1].set_xlabel('Word Count')\naxes[0, 1].set_ylabel('Frequency')\naxes[0, 1].legend()\n\n# Statistical comparison\ntrain_stats = train_df[['text_length', 'word_count']].describe()\ntest_stats = test_df[['text_length', 'word_count']].describe()\n\n# Plot means comparison\nmetrics = ['text_length', 'word_count']\ntrain_means = [train_stats.loc['mean', metric] for metric in metrics]\ntest_means = [test_stats.loc['mean', metric] for metric in metrics]\n\nx = np.arange(len(metrics))\nwidth = 0.35\n\naxes[1, 0].bar(x - width/2, train_means, width, label='Train', color='blue', alpha=0.7)\naxes[1, 0].bar(x + width/2, test_means, width, label='Test', color='red', alpha=0.7)\naxes[1, 0].set_title('Mean Values: Train vs Test')\naxes[1, 0].set_xticks(x)\naxes[1, 0].set_xticklabels(metrics)\naxes[1, 0].legend()\n\n# Remove the last subplot\nfig.delaxes(axes[1, 1])\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"10e2dfaf","cell_type":"code","source":"# Summary statistics comparison\nprint(\" Train vs Test Statistics Comparison:\")\nprint(\"=\"*50)\nprint(\"\\nCharacter Length:\")\nprint(f\"Train - Mean: {train_df['text_length'].mean():.2f}, Std: {train_df['text_length'].std():.2f}\")\nprint(f\"Test  - Mean: {test_df['text_length'].mean():.2f}, Std: {test_df['text_length'].std():.2f}\")\n\nprint(\"\\nWord Count:\")\nprint(f\"Train - Mean: {train_df['word_count'].mean():.2f}, Std: {train_df['word_count'].std():.2f}\")\nprint(f\"Test  - Mean: {test_df['word_count'].mean():.2f}, Std: {test_df['word_count'].std():.2f}\")\n\n# Check for data quality issues\nprint(\"\\n Data Quality Assessment:\")\nprint(\"=\"*30)\n\n# Check for very short texts\nshort_texts_train = train_df[train_df['text_length'] < 10]\nshort_texts_test = test_df[test_df['text_length'] < 10]\nprint(f\"Very short texts (< 10 chars) - Train: {len(short_texts_train)}, Test: {len(short_texts_test)}\")\n\n# Check for very long texts\nlong_texts_train = train_df[train_df['text_length'] > 500]\nlong_texts_test = test_df[test_df['text_length'] > 500]\nprint(f\"Very long texts (> 500 chars) - Train: {len(long_texts_train)}, Test: {len(long_texts_test)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"0ab5a4b7","cell_type":"markdown","source":"## 6. Key Insights and Recommendations","metadata":{}},{"id":"05b854ad","cell_type":"code","source":"print(\" KEY INSIGHTS FROM EDA:\")\nprint(\"=\"*50)\nprint(f\"1. Dataset Size: {train_df.shape[0]} training samples, {test_df.shape[0]} test samples\")\n\nif 'sentiment' in train_df.columns:\n    print(f\"2. Sentiment Distribution:\")\n    for sentiment, count in train_df['sentiment'].value_counts().items():\n        percentage = (count / len(train_df)) * 100\n        print(f\"   - {sentiment}: {count} ({percentage:.1f}%)\")\n\nprint(f\"\\n3. Text Characteristics:\")\nprint(f\"   - Average text length: {train_df['text_length'].mean():.1f} characters\")\nprint(f\"   - Average word count: {train_df['word_count'].mean():.1f} words\")\nprint(f\"   - Text length range: {train_df['text_length'].min()} - {train_df['text_length'].max()} characters\")\n\nprint(f\"\\n4. Data Quality:\")\nprint(f\"   - Missing values: {train_df.isnull().sum().sum()} in train, {test_df.isnull().sum().sum()} in test\")\nprint(f\"   - Duplicates: {train_df.duplicated().sum()} in train, {test_df.duplicated().sum()} in test\")\n\nprint(f\"\\n5. Train vs Test Comparison:\")\nprint(f\"   - Text length difference: {abs(train_df['text_length'].mean() - test_df['text_length'].mean()):.2f} chars\")\nprint(f\"   - Word count difference: {abs(train_df['word_count'].mean() - test_df['word_count'].mean()):.2f} words\")\n\nprint(f\"\\n RECOMMENDATIONS:\")\nprint(\"=\"*30)\nprint(\"1. Text Preprocessing:\")\nprint(\"   - Remove special characters and normalize text\")\nprint(\"   - Handle case sensitivity\")\nprint(\"   - Consider removing very short/long texts if they're outliers\")\n\nprint(\"\\n2.  Feature Engineering:\")\nprint(\"   - Extract text length and word count as features\")\nprint(\"   - Consider TF-IDF or word embeddings\")\nprint(\"   - Analyze n-grams for sentiment patterns\")\n\nprint(\"\\n3.  Model Considerations:\")\nif 'sentiment' in train_df.columns:\n    sentiment_counts = train_df['sentiment'].value_counts()\n    is_balanced = max(sentiment_counts) / min(sentiment_counts) < 2\n    if not is_balanced:\n        print(\"   - Handle class imbalance with sampling techniques\")\n    else:\n        print(\"   - Classes are reasonably balanced\")\nprint(\"   - Use stratified split for validation\")\nprint(\"   - Consider ensemble methods for better performance\")\n\nprint(\"\\n✅ EDA Complete! Ready for model development.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5d6e193b","cell_type":"markdown","source":"## 7. Model Development - DeBERTa for Sentiment Analysis","metadata":{}},{"id":"4fc54b4e","cell_type":"code","source":"# Install required packages\n!pip install transformers torch scikit-learn datasets accelerate tqdm -q\n\n# Import additional libraries for modeling\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    TrainingArguments, \n    Trainer, \n    DataCollatorWithPadding,\n    pipeline\n)\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nimport torch\nfrom torch.utils.data import DataLoader\nimport gc\nfrom tqdm.auto import tqdm\n\nprint(\" Model libraries imported successfully!\")\nprint(f\" CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\" GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\" GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n    \n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\" Using device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"6bfcbac0","cell_type":"code","source":"# Data preprocessing for DeBERTa\ndef preprocess_text(text):\n    \"\"\"Clean and preprocess text for DeBERTa\"\"\"\n    if pd.isna(text):\n        return \"\"\n    \n    # Convert to string\n    text = str(text)\n    \n    # Basic cleaning while preserving important sentiment indicators\n    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n    text = re.sub(r'@\\w+', '', text)     # Remove mentions\n    text = re.sub(r'#(\\w+)', r'\\1', text)  # Remove # but keep hashtag words\n    text = re.sub(r'\\s+', ' ', text)     # Remove extra whitespace\n    text = text.strip()\n    \n    return text\n\n# Preprocess the text data\nprint(\"🧹 Preprocessing text data...\")\ntrain_df['processed_text'] = train_df['text'].apply(preprocess_text)\ntest_df['processed_text'] = test_df['text'].apply(preprocess_text)\n\n# Create label mapping\nif 'sentiment' in train_df.columns:\n    # Handle missing values and get unique sentiments\n    unique_sentiments = train_df['sentiment'].dropna().unique()\n    unique_sentiments = sorted([str(s) for s in unique_sentiments])  # Convert to string and sort\n    \n    print(f\" Found sentiments: {unique_sentiments}\")\n    \n    # Check for missing values in sentiment column\n    missing_sentiments = train_df['sentiment'].isnull().sum()\n    if missing_sentiments > 0:\n        print(f\" Warning: {missing_sentiments} missing sentiment labels found!\")\n        # Drop rows with missing sentiment labels\n        train_df = train_df.dropna(subset=['sentiment'])\n        print(f\" After removing missing labels: {len(train_df)} samples remain\")\n    \n    label2id = {label: i for i, label in enumerate(unique_sentiments)}\n    id2label = {i: label for label, i in label2id.items()}\n    \n    print(f\" Label mapping: {label2id}\")\n    \n    # Convert sentiments to numerical labels\n    train_df['labels'] = train_df['sentiment'].map(label2id)\n    \n    print(f\" Preprocessing complete!\")\n    print(f\" Training samples: {len(train_df)}\")\n    print(f\" Test samples: {len(test_df)}\")\nelse:\n    print(\" No sentiment column found in training data\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d9abbd0b","cell_type":"code","source":"# Model configuration\nMODEL_NAME = \"microsoft/deberta-v3-base\"  # DeBERTa v3 base model\nMAX_LENGTH = 512\nBATCH_SIZE = 8  # Adjust based on GPU memory\nLEARNING_RATE = 2e-5\nNUM_EPOCHS = 3\n\nprint(f\" Loading {MODEL_NAME}...\")\n\n# Initialize tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\n# Initialize model\nif 'sentiment' in train_df.columns:\n    num_labels = len(unique_sentiments)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        MODEL_NAME,\n        num_labels=num_labels,\n        id2label=id2label,\n        label2id=label2id\n    )\n    \n    # Move model to GPU if available\n    model = model.to(device)\n    print(f\" Model loaded with {num_labels} classes: {list(label2id.keys())}\")\n    print(f\" Model moved to: {next(model.parameters()).device}\")\nelse:\n    print(\" Cannot initialize model without sentiment labels\")\n\n# Tokenization function\ndef tokenize_function(examples):\n    return tokenizer(\n        examples['processed_text'],\n        truncation=True,\n        padding=True,\n        max_length=MAX_LENGTH,\n        return_tensors=\"pt\"\n    )\n\nprint(f\" Model configuration:\")\nprint(f\"   - Max length: {MAX_LENGTH}\")\nprint(f\"   - Batch size: {BATCH_SIZE}\")\nprint(f\"   - Learning rate: {LEARNING_RATE}\")\nprint(f\"   - Epochs: {NUM_EPOCHS}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"4b2857fc","cell_type":"code","source":"# GPU Verification and Optimization\nprint(\" GPU Setup and Verification:\")\nprint(\"=\"*35)\n\nif torch.cuda.is_available():\n    print(f\" CUDA is available!\")\n    print(f\" Device: {torch.cuda.get_device_name(0)}\")\n    print(f\" Total Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n    print(f\" CUDA Version: {torch.version.cuda}\")\n    \n    # Clear cache and optimize\n    torch.cuda.empty_cache()\n    \n    # Set memory optimization\n    if hasattr(torch.cuda, 'set_memory_fraction'):\n        torch.cuda.set_memory_fraction(0.9)  # Use 90% of GPU memory\n        print(\" Memory fraction set to 90%\")\n    \n    # Enable optimized attention if available\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    print(\" TensorFloat-32 optimizations enabled\")\n    \n    print(f\" Available Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\nelse:\n    print(\" CUDA not available - training will use CPU\")\n    print(\" This will be significantly slower!\")\n\nprint(f\"\\n Final device: {device}\")\nprint(\"=\"*35)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"c0192170","cell_type":"code","source":"# Create train/validation split\nif 'sentiment' in train_df.columns:\n    X_train, X_val, y_train, y_val = train_test_split(\n        train_df['processed_text'].tolist(),\n        train_df['labels'].tolist(),\n        test_size=0.2,\n        random_state=42,\n        stratify=train_df['labels']\n    )\n    \n    print(f\" Dataset splits:\")\n    print(f\"   - Training: {len(X_train)} samples\")\n    print(f\"   - Validation: {len(X_val)} samples\")\n    print(f\"   - Test: {len(test_df)} samples\")\n    \n    # Create datasets\n    train_dataset = Dataset.from_dict({\n        'processed_text': X_train,\n        'labels': y_train\n    })\n    \n    val_dataset = Dataset.from_dict({\n        'processed_text': X_val,\n        'labels': y_val\n    })\n    \n    test_dataset = Dataset.from_dict({\n        'processed_text': test_df['processed_text'].tolist()\n    })\n    \n    # Tokenize datasets\n    print(\" Tokenizing datasets...\")\n    train_dataset = train_dataset.map(tokenize_function, batched=True)\n    val_dataset = val_dataset.map(tokenize_function, batched=True)\n    test_dataset = test_dataset.map(tokenize_function, batched=True)\n    \n    # Set format for PyTorch\n    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    val_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n    \n    print(\" Datasets prepared and tokenized!\")\n    \n    # Validation split distribution\n    val_labels = [id2label[label] for label in y_val]\n    val_dist = pd.Series(val_labels).value_counts()\n    print(f\"\\n Validation set distribution:\")\n    for sentiment, count in val_dist.items():\n        percentage = (count / len(y_val)) * 100\n        print(f\"   - {sentiment}: {count} ({percentage:.1f}%)\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"82dee6b4","cell_type":"code","source":"# Training configuration\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    # Calculate accuracy\n    accuracy = accuracy_score(labels, predictions)\n    \n    return {'accuracy': accuracy}\n\n# Data collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=NUM_EPOCHS,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    warmup_steps=100,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=50,\n    eval_strategy=\"epoch\",  # Updated parameter name\n    save_strategy=\"epoch\",\n    report_to=[],\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    learning_rate=LEARNING_RATE,\n    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n    dataloader_drop_last=False,\n    save_total_limit=2,\n    seed=42,\n    # Progress bar and logging improvements\n    disable_tqdm=False,  # Enable tqdm progress bars\n\n    greater_is_better=True,  # For accuracy metric\n)\n\n# Initialize trainer\nif 'sentiment' in train_df.columns:\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        tokenizer=tokenizer,\n        data_collator=data_collator,\n        compute_metrics=compute_metrics,\n    )\n    \n    print(\" Trainer initialized successfully!\")\n    print(f\" Training will run for {NUM_EPOCHS} epochs\")\n    print(f\" Results will be saved to './results'\")\nelse:\n    print(\" Cannot initialize trainer without sentiment labels\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ff3054ee","cell_type":"code","source":"# Train the model\nif 'sentiment' in train_df.columns:\n    print(\" Starting model training...\")\n    print(\"=\"*50)\n    \n    # Clear GPU cache and check memory\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        gc.collect()\n        print(f\" GPU Memory before training:\")\n        print(f\"   - Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n        print(f\"   - Cached: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n    \n    # Verify model is on GPU\n    print(f\" Model device: {next(model.parameters()).device}\")\n    print(f\" Training on: {device}\")\n    \n    # Enable tqdm for progress tracking\n    tqdm.pandas()\n    \n    # Train the model with progress tracking\n    print(\"\\n Training Progress:\")\n    trainer.train()\n    \n    print(\"\\n Training completed!\")\n    \n    # Check GPU memory after training\n    if torch.cuda.is_available():\n        print(f\" GPU Memory after training:\")\n        print(f\"   - Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n        print(f\"   - Cached: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n    \n    # Save the final model\n    trainer.save_model('./best_model')\n    tokenizer.save_pretrained('./best_model')\n    \n    print(\" Model saved to './best_model'\")\nelse:\n    print(\" Cannot train model without sentiment labels\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7ba24f87","cell_type":"code","source":"# Model evaluation\nif 'sentiment' in train_df.columns:\n    print(\" Evaluating model on validation set...\")\n    print(\"=\"*40)\n    \n    # Evaluate on validation set\n    eval_results = trainer.evaluate()\n    \n    print(\" Validation Results:\")\n    for key, value in eval_results.items():\n        if isinstance(value, float):\n            print(f\"   - {key}: {value:.4f}\")\n        else:\n            print(f\"   - {key}: {value}\")\n    \n    # Get predictions on validation set\n    predictions = trainer.predict(val_dataset)\n    y_pred = np.argmax(predictions.predictions, axis=1)\n    y_true = y_val\n    \n    # Detailed classification report\n    print(\"\\n Detailed Classification Report:\")\n    print(\"=\"*40)\n    target_names = [id2label[i] for i in range(len(id2label))]\n    report = classification_report(y_true, y_pred, target_names=target_names)\n    print(report)\n    \n    # Confusion matrix\n    print(\"\\n Confusion Matrix:\")\n    print(\"=\"*25)\n    cm = confusion_matrix(y_true, y_pred)\n    \n    # Create confusion matrix visualization\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n                xticklabels=target_names, yticklabels=target_names)\n    plt.title('Confusion Matrix - DeBERTa Model')\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    plt.tight_layout()\n    plt.show()\n    \n    # Per-class accuracy\n    print(\"\\n Per-class Accuracy:\")\n    print(\"=\"*25)\n    for i, sentiment in enumerate(target_names):\n        class_correct = cm[i, i]\n        class_total = cm[i, :].sum()\n        class_accuracy = class_correct / class_total if class_total > 0 else 0\n        print(f\"   - {sentiment}: {class_accuracy:.3f} ({class_correct}/{class_total})\")\n    \n    # Overall accuracy\n    overall_accuracy = accuracy_score(y_true, y_pred)\n    print(f\"\\n Overall Validation Accuracy: {overall_accuracy:.4f}\")\nelse:\n    print(\" Cannot evaluate model without sentiment labels\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bbed9462","cell_type":"code","source":"# Generate predictions on test set\nif 'sentiment' in train_df.columns:\n    print(\" Generating predictions on test set...\")\n    print(\"=\"*40)\n    \n    # Get predictions on test set\n    test_predictions = trainer.predict(test_dataset)\n    test_pred_labels = np.argmax(test_predictions.predictions, axis=1)\n    \n    # Convert numerical predictions back to sentiment labels\n    test_pred_sentiments = [id2label[pred] for pred in test_pred_labels]\n    \n    # Add predictions to test dataframe\n    test_df['predicted_sentiment'] = test_pred_sentiments\n    \n    print(f\" Generated {len(test_pred_sentiments)} predictions\")\n    \n    # Show prediction distribution\n    pred_dist = pd.Series(test_pred_sentiments).value_counts()\n    print(f\"\\n Test Predictions Distribution:\")\n    for sentiment, count in pred_dist.items():\n        percentage = (count / len(test_pred_sentiments)) * 100\n        print(f\"   - {sentiment}: {count} ({percentage:.1f}%)\")\n    \n    # Show some sample predictions\n    print(f\"\\n Sample Predictions:\")\n    print(\"=\"*30)\n    sample_indices = np.random.choice(len(test_df), size=5, replace=False)\n    for i, idx in enumerate(sample_indices, 1):\n        text = test_df.iloc[idx]['text'][:100]\n        pred = test_df.iloc[idx]['predicted_sentiment']\n        print(f\"{i}. Text: {text}...\")\n        print(f\"   Prediction: {pred}\")\n        print()\nelse:\n    print(\" Cannot generate predictions without trained model\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"83c3251f","cell_type":"code","source":"# Create Kaggle submission file\nif 'sentiment' in train_df.columns and 'predicted_sentiment' in test_df.columns:\n    print(\" Creating Kaggle submission file...\")\n    print(\"=\"*35)\n    \n    # Prepare submission dataframe\n    # Assuming test.csv has an 'id' column for Kaggle submission\n    if 'id' in test_df.columns:\n        submission = pd.DataFrame({\n            'id': test_df['id'],\n            'sentiment': test_df['predicted_sentiment']\n        })\n    else:\n        # If no id column, create one\n        submission = pd.DataFrame({\n            'id': range(len(test_df)),\n            'sentiment': test_df['predicted_sentiment']\n        })\n        print(\" No 'id' column found in test data, created sequential IDs\")\n    \n    # Save submission file\n    submission_filename = 'deberta_sentiment_submission.csv'\n    submission.to_csv(submission_filename, index=False)\n    \n    print(f\"Submission file saved as '{submission_filename}'\")\n    print(f\" Submission shape: {submission.shape}\")\n    print(f\"\\n Submission file preview:\")\n    print(submission.head(10))\n    \n    # Submission statistics\n    print(f\"\\n Submission Statistics:\")\n    print(\"=\"*30)\n    submission_dist = submission['sentiment'].value_counts()\n    for sentiment, count in submission_dist.items():\n        percentage = (count / len(submission)) * 100\n        print(f\"   - {sentiment}: {count} ({percentage:.1f}%)\")\n    \n    print(f\"\\n Model Summary:\")\n    print(\"=\"*20)\n    print(f\"   - Model: {MODEL_NAME}\")\n    print(f\"   - Validation Accuracy: {overall_accuracy:.4f}\")\n    print(f\"   - Training Epochs: {NUM_EPOCHS}\")\n    print(f\"   - Max Length: {MAX_LENGTH}\")\n    print(f\"   - Batch Size: {BATCH_SIZE}\")\n    print(f\"   - Learning Rate: {LEARNING_RATE}\")\n    \n    print(f\"\\n Ready for Kaggle submission!\")\n    print(f\" Submit file: {submission_filename}\")\nelse:\n    print(\" Cannot create submission file without predictions\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}